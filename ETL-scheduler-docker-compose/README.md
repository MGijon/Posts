# ETL Orchestrator Benchmarking Project

This project aims to evaluate and compare various lightweight (and heavyweight) orchestrators to determine the best fit for an MVP transition from Docker Compose to k3s. We measure resource consumption (CPU/RAM) and observability ease for each candidate.

## üìã Methodology

To ensure accurate results, we follow a strict "Clean Slate" protocol between tests:

* **Reset Environment**: Remove all containers, networks, and volumes.
* **Execution**: Start the specific orchestrator stack.
* **Benchmark**: Run the 5-minute resource capture script.
* **Analyze**: Review logs and generated CSV data.


```bash
# Clean slate command
docker system prune -a --volumes -f
```

## üöÄ Orchestrator Execution Matrix

| Orquestrator | Execute scheduler | Observation method | 
| :----------: | :---------------: | :----------------: | 
| Ofelia | ```docker-compose -f ofelia-test.yml up -d``` | ```docker logs -f ofelia_scheduler``` |
| Supervisord | ```docker-compose -f supervisor-test.yml up -d``` | ```docker logs -f supervisor_etl``` |
| Cronicle | ```docker-compose -f cronicle-test.yml up -d``` | Web UI at ```http://localhost:8081``` |
| Airflow | ```docker-compose -f airflow-test.yml up -d``` | Web UI at ```http://localhost:8080``` |
| Kestra | ```docker-compose -f kestra-test.yml up -d``` | <ul><li>Web UI at ```http://localhost:8082```</li><li>Create a volume ```dags/``` for keeping dags' definitions</li></ul> |
<!-- | Dagster | ```docker-compose -f dagster-test.yml up -d``` | Web UI at ```http://localhost:3000``` | I can not make it work on Mac -->

## üìâ Benchmarking Workflow

1. Initialize Stack
Choose an orchestrator and bring it up in detached mode.

```bash
docker-compose -f [filename].yml up -d
```

2. Run Automated Capture
Execute the benchmark script. Ensure the stack runs for at least 5 minutes to capture multiple cron fires.


```bash
chmod +x benchmark.sh
./benchmark.sh results/[orchestrator].csv
```

3. Cleanup
Bash

```bash
docker-compose -f [filename].yml down
```

## üìä Results Summary

Content at ```results/final_summary_report.csv``` and generated by executing ```analysis.py```.

| Orchestrator        |   Idle_RAM_MiB |   Peak_RAM_MiB |   Avg_RAM_MiB |   RAM_Per_Job |
|:--------------------|---------------:|---------------:|--------------:|--------------:|
| Results/supervisord |          19.26 |          19.61 |         19.49 |          3.92 |
| Results/ofelia      |           9.08 |          47.49 |         34.5  |          9.5  |
| Results/cronicle    |          50.4  |          51.64 |         51.03 |         10.33 |
| Results/kestra      |        1085.44 |        1295.36 |       1273.53 |        259.07 |
| Results/airflow     |        1431.42 |        1446.05 |       1438.3  |        289.21 |


### Key Performance Indicators (KPIs)

To evaluate these tools objectively, we look beyond the marketing features and focus on three metrics that define the operational reality of a startup MVP:

* **Idle Footprint**: This measures the baseline resource consumption when no tasks are running.

    * Importance: On a $5/month VPS (usually 1GB RAM), a 1.4GB Airflow tax is a dealbreaker. We prioritize tools that leave the majority of system resources available for the actual data processing.

* **Audit Trail & Observability**: The "3 AM Test." How many clicks does it take to find the exact error log of a job that failed 10 iterations ago?

    * Importance: Supervisord and Ofelia fail here (logs are ephemeral or stdout-only), while Cronicle and Kestra provide dedicated history databases with persistency.

* **Polyglot Readiness**: The ease of executing non-Python binaries (Rust, Node.js, Go).

    * Importance: We assess if the orchestrator treats the script as a "containerized command" (highly agnostic) or requires a language-specific SDK/Wrapper (low agnosticism).

## üêç Python Analysis Setup

The benchmarking analysis and visualization scripts require a specific Python environment to handle the data processing and graph generation.

### 1. Environment Setup

We use a virtual environment to keep dependencies isolated and pip to manage packages.

```bash
# Create the virtual environment
python3 -m venv .venv

# Activate the environment
# On macOS/Linux:
source .venv/bin/activate
# On Windows:
# .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Code Quality & Formatting

To maintain a professional standard, this project uses Black for uncompromising code formatting. Before committing any changes to the analysis scripts, run:

```bash
# Format all python files in the directory
black .
```

### 3. Running the Analysis

Once your results/ folder is populated with CSV data from the benchmarks, generate the final report and charts:

```bash
python analysis.py               # main analysis of the results
python normalize_performance.py  # generate a normalize version with T=0 so is easy to interpreter
python generate_table_img.py     # generates an image of the table with the raw results
```

---

## ü§ù Contributing

Contributions are welcome! Whether you want to fix a bug, improve the analysis scripts, or add a new orchestrator to the benchmark, follow the steps below.

### Adding a New Orchestrator

If you'd like to see how another tool (e.g., Prefect, Mage, or Argo) compares to these results, please follow this template:

* **Docker Compose**: Provide a docker-compose.yml that stands up the tool in a "minimal" configuration.
* **Standard Task**: Configure the tool to run the included etl_task.py script every 60 seconds.
* **Run Benchmark**: Use the ./benchmark.sh script to generate a ```results/[tool_name].csv```.
* **Submit a PR**: Include your raw CSV and a brief description of the configuration used.

### Development Standards

To keep the repository clean and the analysis reliable:

* **Python Formatting**: We use Black. Please run black . before committing.
* **Data Integrity**: Do not manually edit the CSV files generated by the benchmark script.
* **Environment**: Ensure you update requirements.txt if you add new analysis libraries.

### Roadmap

* [ ] Add "Scheduling Latency" metrics (Time from trigger to stdout).
* [ ] Benchmarks for K3s/Kubernetes resource usage.
* [ ] Integration tests for Rust/Wasm tasks.

## Acknowledgments

A huge thank you to the r/mlops subreddit for being the sounding board for this project. The original discussion sparked the "Clean Slate" methodology used here and pushed me to look beyond the industry giants toward more efficient, Docker-native alternatives. You can find the original thread and the community's early feedback [here](https://www.reddit.com/r/mlops/comments/1qbte3c/seeking_a_lightweight_orchestrator_for_docker/).