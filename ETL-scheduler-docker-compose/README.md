# ETL Orchestrator Benchmarking Project

This project aims to evaluate and compare various lightweight (and heavyweight) orchestrators to determine the best fit for an MVP transition from Docker Compose to k3s. We measure resource consumption (CPU/RAM) and observability ease for each candidate.

## üìã Methodology

To ensure accurate results, we follow a strict "Clean Slate" protocol between tests:

* **Reset Environment**: Remove all containers, networks, and volumes.
* **Execution**: Start the specific orchestrator stack.
* **Benchmark**: Run the 5-minute resource capture script.
* **Analyze**: Review logs and generated CSV data.


```bash
# Clean slate command
docker system prune -a --volumes -f
```

## üöÄ Orchestrator Execution Matrix

| Orquestrator | Execute scheduler | Observation method | 
| :----------: | :---------------: | :----------------: | 
| Ofelia | ```docker-compose -f ofelia-test.yml up -d``` | ```docker logs -f ofelia_scheduler``` |
| Supervisord | ```docker-compose -f supervisor-test.yml up -d``` | ```docker logs -f supervisor_etl``` |
| Cronicle | ```docker-compose -f cronicle-test.yml up -d``` | Web UI at ```http://localhost:8081``` |
| Airflow | ```docker-compose -f airflow-test.yml up -d``` | Web UI at ```http://localhost:8080``` |
| Kestra | ```docker-compose -f kestra-test.yml up -d``` | <ul><li>Web UI at ```http://localhost:8082```</li><li>Create a volume ```dags/``` for keeping dags' definitions</li></ul> |
<!-- | Dagster | ```docker-compose -f dagster-test.yml up -d``` | Web UI at ```http://localhost:3000``` | I can not make it work on Mac -->

## üìâ Benchmarking Workflow

1. Initialize Stack
Choose an orchestrator and bring it up in detached mode.

```bash
docker-compose -f [filename].yml up -d
```

2. Run Automated Capture
Execute the benchmark script. Ensure the stack runs for at least 5 minutes to capture multiple cron fires.


```bash
chmod +x benchmark.sh
./benchmark.sh results/[orchestrator].csv
```

3. Cleanup
Bash

```bash
docker-compose -f [filename].yml down
```

## üìä Results Summary

Content at ```results/final_summary_report.csv``` and generated by executing ```analysis.py```.

| Orchestrator        |   Idle_RAM_MiB |   Peak_RAM_MiB |   Avg_RAM_MiB |   RAM_Per_Job |
|:--------------------|---------------:|---------------:|--------------:|--------------:|
| Results/supervisord |          19.26 |          19.61 |         19.49 |          3.92 |
| Results/ofelia      |           9.08 |          47.49 |         34.5  |          9.5  |
| Results/cronicle    |          50.4  |          51.64 |         51.03 |         10.33 |
| Results/kestra      |        1085.44 |        1295.36 |       1273.53 |        259.07 |
| Results/airflow     |        1431.42 |        1446.05 |       1438.3  |        289.21 |


### Key Performance Indicators (KPIs)

To evaluate these tools objectively, we look beyond the marketing features and focus on three metrics that define the operational reality of a startup MVP:

* **Idle Footprint**: This measures the baseline resource consumption when no tasks are running.

    * Importance: On a $5/month VPS (usually 1GB RAM), a 1.4GB Airflow tax is a dealbreaker. We prioritize tools that leave the majority of system resources available for the actual data processing.

* **Scheduling Latency**: The delta between the scheduled "Cron time" and the actual process "Start time."

    * Importance: In "black-box" schedulers like Ofelia, latency is near-zero. In heavyweight orchestrators, internal polling loops and worker availability can introduce a delay of several seconds, which is critical for time-sensitive ETL.

* **Audit Trail & Observability**: The "3 AM Test." How many clicks does it take to find the exact error log of a job that failed 10 iterations ago?

    * Importance: Supervisord and Ofelia fail here (logs are ephemeral or stdout-only), while Cronicle and Kestra provide dedicated history databases with persistency.

* **Polyglot Readiness**: The ease of executing non-Python binaries (Rust, Node.js, Go).

    * Importance: We assess if the orchestrator treats the script as a "containerized command" (highly agnostic) or requires a language-specific SDK/Wrapper (low agnosticism).

## üêç Python Analysis Setup

The benchmarking analysis and visualization scripts require a specific Python environment to handle the data processing and graph generation.

### 1. Environment Setup

We use a virtual environment to keep dependencies isolated and pip to manage packages.

```bash
# Create the virtual environment
python3 -m venv .venv

# Activate the environment
# On macOS/Linux:
source .venv/bin/activate
# On Windows:
# .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Code Quality & Formatting

To maintain a professional standard, this project uses Black for uncompromising code formatting. Before committing any changes to the analysis scripts, run:

```bash
# Format all python files in the directory
black .
```

### 3. Running the Analysis

Once your results/ folder is populated with CSV data from the benchmarks, generate the final report and charts:

```bash
python analysis.py
python normalize_performance.py
```